{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee7defa",
   "metadata": {},
   "source": [
    "### Project - Historical Product Demand\n",
    "\n",
    "#### Problem definition : \n",
    "\n",
    "- As stated in the notes for this dataset - the problem is that this customer has many warehouses across the globe across many product IDs. \n",
    "- It takes one month to ship product to a warehouse location.\n",
    "\n",
    "#### Goal : \n",
    "- To create accurate forecasting across warehouses and product ID so inventory can be optimized - resulting in no loss in sales and/or no excess production. \n",
    "\n",
    "#### How is it being solved today : \n",
    "- Since we dont have access to the original stakeholders, the assumption is a manual/instinctive forecasting being applied which may be resulting in inefficiencies(Gaps). \n",
    "\n",
    "#### Metrics for success : \n",
    "- By hypothesis testing we will confirm a 95% confidence interval between the distributions of the actual test values (lets say 10% of the tail end of the data) and our predicted results. \n",
    "- We will also compare the results to a Simple Moving Average and/or Exponential Weighted Moving Average\n",
    "- We will also report the Mean Absolute Error, Mean Squared Error and Root Mean Squared Error in context of the mean of the distribution.\n",
    "\n",
    "#### Actions to be taken based on this work : \n",
    "- Optimize inventory at each location using forecasts.\n",
    "- Any further actions that become evident during Exploratory Data Analysis or other steps. \n",
    "\n",
    "#### Scope of the project :\n",
    "- Data Collection - Not Required\n",
    "- Analysis - Required\n",
    "- Observation and Reporting - Required\n",
    "- Prediction - Required\n",
    "- Actionalable insights - Required. Aim to identify seasonal trends, demand spikes and underperforming products.\n",
    "- Deployment - Not Required\n",
    "- Retraining Pipelines - Required\n",
    "- Visualisations - Required\n",
    "\n",
    "#### Timelines : \n",
    "- High Level Timeline estimate - Not Required for Sample project\n",
    "- Granular Task and Timelines breakdown - Not Required for Sample project\n",
    "\n",
    "#### Ethical & Regulatory considerations for this project : None\n",
    "\n",
    "#### Data Collection  : Not required for Sample project\n",
    "- Integration of different data sources\n",
    "- Data Privacy and Compliance\n",
    "- Data Storage and Security\n",
    "- Data Accessibility\n",
    "\n",
    "#### Project Steps\t\n",
    "- Data Exploration, Cleaning and Preparation\n",
    "- Feature Engineering\n",
    "- Feature Selection\n",
    "- Model Building\n",
    "- Model Evaluation\n",
    "\n",
    "#### Deployment\t: Not Required for Sample project\n",
    "- Model Deployment\n",
    "- Version Control\n",
    "- Model Monitoring and Maintenance Plan\n",
    "- Scalability Considerations\n",
    "- Automated Testing\n",
    "\n",
    "#### Communicate and Reporting\n",
    "- Report Findings - Required\n",
    "- Stakeholder Presentations - Not required for Sample project\n",
    "- Create Dashboards for interactive reporting - Not required for Sample project\n",
    "- User Training - Not required for Sample project\n",
    "\t\n",
    "#### Documentation\t\n",
    "- Project steps and methodologies, parameters, deployment details - In Notebook, during project development\n",
    "- Code Documentation during development - In Notebook, during project development\n",
    "- Data Lineage Documentation - Not Required\n",
    "- Model Explanation and Interpretability - In Notebook, during project development\n",
    "\t\n",
    "#### Review\t\n",
    "- Review Metrics of project - Via confirmation of Hypothesis testing, vs baseline SMA and EWMA and common error metrics MAE, MSE and RMSE\n",
    "- Lessons learned - In Notebook, after project development\n",
    "- Feedback Collection - Currently not required. \n",
    "- Follow up actions - Currently not required\n",
    "- Model Decommissioning Plan - Currently not required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e60247",
   "metadata": {},
   "source": [
    "### Project Approach: \n",
    "\n",
    "- Since there are many warehouses and projects, we need to forecast orders demand for each product at each location. \n",
    "- We will break down the dataset into separate dataframes for each warehouse and each product and perform predictions on each. \n",
    "- We need to ensure the code is modular so that in future forecasting can be applied to the entire data again or to an individual warehouse or product. \n",
    "- We will be attempting ARMA, ARIMA and SARIMA models to make our predictions. Failing good results we may try LSTM models but that option is not preferable due to explainability and interpretability. \n",
    "###### Note that at some later point in the business, depending on the success of this project, it may be a good idea to try VAR and VARMA models between different locations of warehouses. The assumption being that proximity between warehouses may affect each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import ks_2samp\n",
    "import joblib\n",
    "from fpdf import FPDF\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the master DataFrame from a CSV file\n",
    "file_name = 'Historical Product Demand.csv'\n",
    "master = pd.read_csv(file_name)\n",
    "\n",
    "# Function to preprocess and aggregate data\n",
    "def preprocess_and_aggregate(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Order_Demand'] = df['Order_Demand'].str.strip('()').astype(float)\n",
    "    df['Month'] = df['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "    aggregated_df = df.groupby(['Month', 'Product_Code', 'Warehouse', 'Product_Category'])['Order_Demand'].sum().reset_index()\n",
    "    return aggregated_df\n",
    "\n",
    "# Function to create a time series with all months filled\n",
    "def create_filled_time_series(df):\n",
    "    min_date = df['Month'].min()\n",
    "    max_date = df['Month'].max()\n",
    "    all_months = pd.date_range(start=min_date, end=max_date, freq='MS')\n",
    "    df.set_index('Month', inplace=True)\n",
    "    df = df.reindex(all_months, fill_value=0).rename_axis('Month').reset_index()\n",
    "    return df\n",
    "\n",
    "# Preprocess and aggregate the data\n",
    "preprocessed_master = preprocess_and_aggregate(master)\n",
    "\n",
    "# Function to run full analysis on a given dataframe\n",
    "def run_full_analysis(df, title):\n",
    "    # Check if DataFrame has sufficient length\n",
    "    if len(df) < 24:\n",
    "        print(f\"Skipping {title}: insufficient data length ({len(df)} observations).\")\n",
    "        return None\n",
    "\n",
    "    product_code = title.split(' ')[0]\n",
    "    warehouse = title.split(' ')[2]\n",
    "\n",
    "    # Plot Order Demand\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Month'], df['Order_Demand'])\n",
    "    plt.title(f\"Order Demand for {title}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Order_Demand')\n",
    "    plt.savefig(f'Order_Demand_{product_code}_{warehouse}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Seasonal Decomposition\n",
    "    try:\n",
    "        decomposition = seasonal_decompose(df['Order_Demand'], model='additive', period=12)\n",
    "        decomposition.plot()\n",
    "        plt.savefig(f'Seasonal_Decomposition_{product_code}_{warehouse}.png')\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in seasonal decomposition for {title}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Determine appropriate lags for ACF and PACF plots\n",
    "    nlags = min(40, max(1, len(df) // 2 - 1))\n",
    "\n",
    "    # ACF and PACF plots\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plot_acf(df['Order_Demand'], lags=nlags)\n",
    "        plt.savefig(f'ACF_{product_code}_{warehouse}.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plot_pacf(df['Order_Demand'], lags=nlags)\n",
    "        plt.savefig(f'PACF_{product_code}_{warehouse}.png')\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in ACF/PACF plots for {title}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ADF test report\n",
    "    def adf_test(series, title=''):\n",
    "        result = adfuller(series.dropna(), autolag='AIC')\n",
    "        labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "        out = pd.Series(result[0:4], index=labels)\n",
    "        for key, val in result[4].items():\n",
    "            out[f'critical value ({key})'] = val\n",
    "\n",
    "        print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "        print(out.to_string())\n",
    "\n",
    "        if result[1] <= 0.05:\n",
    "            conclusion = \"Strong evidence against the null hypothesis. Reject the null hypothesis. Data has no unit root and is stationary.\"\n",
    "        else:\n",
    "            conclusion = \"Weak evidence against the null hypothesis. Fail to reject the null hypothesis. Data has a unit root and is non-stationary.\"\n",
    "        \n",
    "        print(conclusion)\n",
    "        return out, conclusion\n",
    "\n",
    "    adf_results, adf_conclusion = adf_test(df['Order_Demand'], title=f'{title}')\n",
    "\n",
    "    # Split train and test\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train, test = df[:train_size], df[train_size:]\n",
    "\n",
    "    if train['Order_Demand'].isnull().all() or test['Order_Demand'].isnull().all():\n",
    "        print(f\"Insufficient data for {title} after splitting. Skipping this combination.\")\n",
    "        return None\n",
    "\n",
    "    # Fit SARIMA model with specified parameters\n",
    "    order = (0, 0, 0)\n",
    "    seasonal_order = (0, 2, 3, 12)\n",
    "    try:\n",
    "        model = SARIMAX(train['Order_Demand'], order=order, seasonal_order=seasonal_order)\n",
    "        model_fit = model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting SARIMAX model for {title}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Forecast for the test period\n",
    "    forecast = model_fit.get_forecast(steps=len(test))\n",
    "    forecast_mean = forecast.predicted_mean  # These are the predicted values for the test period\n",
    "    forecast_ci = forecast.conf_int()  # Confidence intervals for the predicted values\n",
    "\n",
    "    # Forecast for the next 12 months\n",
    "    forecast_12_months = model_fit.get_forecast(steps=12)\n",
    "    forecast_12_mean = forecast_12_months.predicted_mean  # Predicted values for the next 12 months\n",
    "    forecast_12_ci = forecast_12_months.conf_int()  # Confidence intervals for the next 12 months\n",
    "\n",
    "    # Forecast for the next 1 month\n",
    "    forecast_1_month = model_fit.get_forecast(steps=1)\n",
    "    forecast_1_mean = forecast_1_month.predicted_mean.iloc[0]  # Predicted value for the next 1 month\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(test['Order_Demand'], forecast_mean)\n",
    "    mse = mean_squared_error(test['Order_Demand'], forecast_mean)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mean_val = np.mean(test['Order_Demand'])\n",
    "    std_val = np.std(test['Order_Demand'])\n",
    "    mse_mean_ratio = mse / mean_val\n",
    "    rmse_std_ratio = rmse / std_val\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_p_value = ks_2samp(test['Order_Demand'], forecast_mean)\n",
    "    print(f\"KS Test Value: {ks_stat}, KS p-Value: {ks_p_value}\")\n",
    "\n",
    "    # Determine status based on KS test p-value\n",
    "    if ks_p_value >= 0.7:\n",
    "        status = 'deploy'\n",
    "    elif 0.3 <= ks_p_value < 0.7:\n",
    "        status = 'deploy and monitor'\n",
    "    else:\n",
    "        status = 'retrain'\n",
    "\n",
    "    # Save model if status is deploy or deploy and monitor\n",
    "    if status in ['deploy', 'deploy and monitor']:\n",
    "        model_name = f'{warehouse}{product_code[8:]}261222.pkl'\n",
    "        joblib.dump(model_fit, os.path.join('HPD_models', model_name))\n",
    "\n",
    "    # Metrics DataFrame\n",
    "    metrics = {\n",
    "        'Product_code': product_code,\n",
    "        'Warehouse': warehouse,\n",
    "        '1 Month Forecast': forecast_1_mean,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'Mean': mean_val,\n",
    "        'STD': std_val,\n",
    "        'MSE/Mean': mse_mean_ratio,\n",
    "        'RMSE/STD': rmse_std_ratio,\n",
    "        'KS Test Value': ks_stat,\n",
    "        'KS p-Value': ks_p_value,\n",
    "        'Status': status\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "\n",
    "    # Plot Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train['Month'], train['Order_Demand'], label='Train')\n",
    "    plt.plot(test['Month'], test['Order_Demand'], label='Test')\n",
    "    plt.plot(test['Month'], forecast_mean, label='Predicted')\n",
    "    plt.fill_between(test['Month'], forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.1)\n",
    "    plt.legend()\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.savefig(f'Actual_vs_Predicted_{product_code}_{warehouse}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and Plot Simple Moving Average and Exponential Weighted Moving Average\n",
    "    sma = df['Order_Demand'].rolling(window=12).mean()\n",
    "    ewma = df['Order_Demand'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test['Month'], test['Order_Demand'], label='Actual Test')\n",
    "    plt.plot(test['Month'], forecast_mean, label='Predicted')\n",
    "    plt.plot(df['Month'], sma, label='Simple Moving Average')\n",
    "    plt.plot(df['Month'], ewma, label='Exponential Weighted Moving Average')\n",
    "    plt.legend()\n",
    "    plt.title('Test, Predicted with SMA and EWMA')\n",
    "    plt.savefig(f'Test_vs_SMA_EWMA_{product_code}_{warehouse}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Month'], df['Order_Demand'], label='History')\n",
    "    plt.plot(pd.date_range(start=test['Month'].iloc[-1] + pd.DateOffset(months=1), periods=12, freq='MS'), forecast_12_mean, label='Forecast')\n",
    "    plt.fill_between(pd.date_range(start=test['Month'].iloc[-1] + pd.DateOffset(months=1), periods=12, freq='MS'), forecast_12_ci.iloc[:, 0], forecast_12_ci.iloc[:, 1], color='k', alpha=0.1)\n",
    "    plt.legend()\n",
    "    plt.title('12-Month Forecast')\n",
    "    plt.savefig(f'12_Month_Forecast_{product_code}_{warehouse}.png')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_df, adf_results, adf_conclusion\n",
    "\n",
    "# Initialize a list to collect metrics dataframes\n",
    "all_metrics = []\n",
    "monthly_forecast = {}\n",
    "\n",
    "# Initialize counter\n",
    "counter = 1\n",
    "\n",
    "# Process and store each dataframe\n",
    "for (product_code, warehouse), group in preprocessed_master.groupby(['Product_Code', 'Warehouse']):\n",
    "    filled_df = create_filled_time_series(group)\n",
    "    result = run_full_analysis(filled_df, f'{product_code} in {warehouse}...Sequential Number {counter}')\n",
    "    if result is not None:\n",
    "        metrics_df, adf_results, adf_conclusion = result\n",
    "        all_metrics.append(metrics_df)\n",
    "        if product_code not in monthly_forecast:\n",
    "            monthly_forecast[product_code] = {}\n",
    "        monthly_forecast[product_code][warehouse] = metrics_df['1 Month Forecast'].values[0]\n",
    "    counter += 1\n",
    "\n",
    "# Concatenate all metrics dataframes\n",
    "final_metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_values = len(final_metrics_df)\n",
    "deploy_values = len(final_metrics_df[final_metrics_df['Status'] == 'deploy'])\n",
    "deploy_and_monitor_values = len(final_metrics_df[final_metrics_df['Status'] == 'deploy and monitor'])\n",
    "retrain_values = len(final_metrics_df[final_metrics_df['Status'] == 'retrain'])\n",
    "\n",
    "summary_statistics = {\n",
    "    'Total Values': total_values,\n",
    "    'Deploy Values': deploy_values,\n",
    "    'Deploy Values %': deploy_values / total_values,\n",
    "    'Deploy and Monitor Values': deploy_and_monitor_values,\n",
    "    'Deploy and Monitor Values %': deploy_and_monitor_values / total_values,\n",
    "    'Retrain Values': retrain_values,\n",
    "    'Retrain Values %': retrain_values / total_values\n",
    "}\n",
    "\n",
    "summary_statistics_df = pd.DataFrame(summary_statistics, index=[0])\n",
    "\n",
    "# Create the monthly forecast table\n",
    "monthly_forecast_df = pd.DataFrame(monthly_forecast).T\n",
    "monthly_forecast_df['Total'] = monthly_forecast_df.sum(axis=1)\n",
    "\n",
    "# Save the reports to PDF\n",
    "reports_dir = 'Reports'\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.makedirs(reports_dir)\n",
    "\n",
    "# Function to generate a PDF report\n",
    "def generate_pdf_report(product_code, warehouse, adf_results, adf_conclusion):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=f\"Sample Report for Product Code: {product_code} Warehouse: {warehouse}\", ln=True, align='C')\n",
    "\n",
    "    # Add plots\n",
    "    for plot_name in ['Order_Demand', 'Seasonal_Decomposition', 'ACF', 'PACF', 'Actual_vs_Predicted', 'Test_vs_SMA_EWMA', '12_Month_Forecast']:\n",
    "        pdf.image(f'{plot_name}_{product_code}_{warehouse}.png', x=10, w=180)\n",
    "        pdf.ln(85)\n",
    "\n",
    "    # Add ADF test results\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    pdf.cell(200, 10, txt=\"ADF Test Results\", ln=True, align='L')\n",
    "    for label, value in adf_results.items():\n",
    "        pdf.cell(200, 10, txt=f\"{label}: {value}\", ln=True, align='L')\n",
    "    pdf.cell(200, 10, txt=adf_conclusion, ln=True, align='L')\n",
    "\n",
    "    # Save the PDF\n",
    "    pdf.output(os.path.join(reports_dir, f'Sample Report for Product Code {product_code} Warehouse {warehouse}.pdf'))\n",
    "\n",
    "# Generate the sample report for the first product_code and warehouse\n",
    "if all_metrics:\n",
    "    first_metrics_df = all_metrics[0]\n",
    "    generate_pdf_report(first_metrics_df['Product_code'].iloc[0], first_metrics_df['Warehouse'].iloc[0], adf_results, adf_conclusion)\n",
    "\n",
    "# Save final metrics and summary statistics to PDF\n",
    "final_metrics_df.to_csv(os.path.join(reports_dir, 'Final Metrics.csv'))\n",
    "summary_statistics_df.to_csv(os.path.join(reports_dir, 'Model Performance Summaries.csv'))\n",
    "monthly_forecast_df.to_csv(os.path.join(reports_dir, 'Monthly Forecast.csv'))\n",
    "\n",
    "# Output the final metrics dataframe and summary statistics\n",
    "print(final_metrics_df)\n",
    "print(summary_statistics_df)\n",
    "print(monthly_forecast_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac10b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94daa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b899c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b8e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cce2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed5c48b",
   "metadata": {},
   "source": [
    "### Review: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Notes and observations: \n",
    "- It may be a good idea to run VAR or VARMA models on the data with multiple warehouse locations orders being the multivariate feature to be forecast. The assumption being that proximity between warehouses may affect each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fda59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
